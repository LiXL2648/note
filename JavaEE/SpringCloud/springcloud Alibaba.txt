十七、Spring Cloud Alibaba
	1. 为什么会出现Spring Cloud Alibaba
		① Spring Cloud Netflix项目进入维护模式：官网https://spring.io/blog/2018/12/12/spring-cloud-greenwich-rc1-available-now说明。
		② Spring Cloud Netflix Projects Entering Maintenance Mode
			A. 什么是维护模式：将模块置于维护模式，意味着Spring Cloud团队将不会向模块添加功能。我们将修复block级别的bug以及安全问题，我们也会考虑并审查社区的小型pull request。
			B. 进入维护模式意味着什么
				a. Spring Cloud Netflix将不再开发新的组件：我们都知道Spring Cloud版本迭代速度比较快，因而出现了很对重大ISSUE都还来不及Fix就又推出另一个RELEASE了。进入维护模式
				意思就是目前和以后的一段时间里Spring Cloud Netflix提供的服务和功能就这么多了，不再开发洗的组件和功能了。以后将维护和Merge分支Full Request为主。
				b. 新组件功能将以其他替代平台代替的方式实现。
	2. Spring Cloud Alibaba 带来了什么
		① Spring Cloud Alibaba核心概念：
			A. GitHub官网：https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md
			B. 诞生：2018.10.31，Spring Cloud Alibaba正式入驻Spring Cloud官方孵化器，并在Maven中央库发布了第一个版本
			C. Spring Cloud Alibaba，它是由一些阿里巴巴的开源组件和云产品组成额，这个项目的目的是为了让大家所熟悉的Spring框架，其优秀的设计模式和抽象概念，以给使用阿里巴巴产品
			的Java开发者带来使用Spring Boot和Spring Cloud的更多便利。
		② Spring Cloud Alibaba作用
			A. 服务限流降级：默认支持Servlet、Feign、RestTemplate、Dubbo和RocketMQ限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级Metrics监控。
			B. 服务注册与发现：适配Spring Cloud服务注册与发现标准，默认继承了Ribbon的支持。
			C. 分布式配置管理：支持分布式系统中的外部化配置，配置更改时自动刷新
			D. 消息驱动能力：基于Spring Cloud Stream为微服务应用构建消息驱动能力
			E. 阿里云对象存储：阿里云提供的海量、安全、低成本、高可靠的云存储服务，支持在任何应用，任何时间，任何地点存储和访问任意类型的数据。
			F. 分布式任务调度：提供秒级、精准、高可靠、高可用的定时（基于cron表达式）任务调度任务，同时提供分布式的任务执行模式，如网格任务。网格任务支持海量子任务均匀分配到所有
			Worker（Schedulerx-client）上执行。
		③ Spring Cloud Alibaba下载地址：https://github.com/alibaba/spring-cloud-alibaba/blob/master/README-zh.md
		④ Spring Cloud Alibaba使用
			A. Sentinel：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。
			B. Nacos：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。
			C. RocketMQ：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。
			D. Dubbo：Apache Dubbo™ 是一款高性能 Java RPC 框架。
			E. Seata：阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。
			F. Alibaba Cloud ACM：一款在分布式架构环境中对应用配置进行集中管理和推送的应用配置中心产品。
			G. Alibaba Cloud OSS: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地
			点存储和访问任意类型的数据。
			H. Alibaba Cloud SchedulerX: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。
			I. Alibaba Cloud SMS: 覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。
	3. Spring Cloud Alibaba学习资料获取
		① 官网：https://spring.io/projects/spring-cloud-alibaba：Spring Cloud Alibaba致力于提供微服务开发一站式解决方案。此项目包含开发分布式微服务的必须组件，方便开发者通过Spring 
		Cloud编程模型轻松使用这些组件来开发分布式应用服务。依托Spring Cloud Alibaba，只需要添加一些注解和少量配置，就可以将Spring Cloud应用接入阿里微服务解决方案，通过阿里中间件来
		迅速搭建分布式应用系统。Spring Cloud Alibaba进入；额Spring Cloud官方孵化器，而且毕业了。
		② 英文：
			A. alibaba/spring-cloud-alibaba
			B. https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba.html
十八、Spring Cloud Alibaba Nacos服务注册和配置中心
	1. Nacos简介
		① 为什么叫Nacos：前四个字母分别为Naming和Configuration的前两字母的组合，最后的s则是Service的首字母。
		② Nacos的概念：
			A. 一个更易于构建云原生应用的动态服务发现，配置管理和服务管理平台。
			B. Nacos：Dynamic Naming and Configuration Service
			C. Nacos就是注册中心+配置中心的组合，等价于Eureka + Config + Bus
		③ Nacos的作用
			A. 替代Eureka作为服务注册中心
			B. 替代Config作为服务配置中心
		④ Nacos的下载地址
			A. 地址：https://github.com/alibaba/Nacos
			B. 官网文档
				a. http://nacos.io/zh-cn/index.html
				b. https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba.html#_spring_clouda_alibaba_nacos_discovery
		⑤ 各注册中心的比较
			----------------------------------------------------------------
			| 服务注册与发现框架 | CAP模型 | 控制台管理 | 社区活跃度	   |
			| Eureka			 | AP	   | 支持 		| 低（2.x版本毕源）|
			| Zookeeper			 | CP	   | 不支持		| 中			   |
			| Consul			 | CP	   | 支持		| 高			   |
			| Nacos				 | AP 	   | 支持	    | 高			   |
			----------------------------------------------------------------
			据说Nocas在阿里巴巴内部有超过10万例运行，已经过了类似于双十一等各种大型流量的考验
	2. 运行并安装Nacos
		① Java8 + Maven
		② 官网下载Nacos
			A. 先从官网下载Nacos
			B. 解压安装包，直接运行Bin目录下的startup.cmd
		③ 使用Docker镜像安装Nacos
			A. 下载nacos镜像：docker pull nacos/nacos-server:1.1.4
			B. 启动Nacos容器：docker run --env MODE=standalone --name nacos -d -p 8848:8848 ac34e13f83a8
		④ 命令运行成功后直接访问http://IP地址:8848/nacos，默认账号密码都是nacos
	3. Nacos作为服务注册中心演示
		① 官方文档：https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba
		② 基于Nacos的服务提供者
			A. 新建Module，cloudalibaba-provider-payment9001和cloudalibaba-provider-payment9002
			B. 改POM
				a. 父POM：
					<dependencyManagement>
						<dependencies>
							<dependency>
								<groupId>com.alibaba.cloud</groupId>
								<artifactId>spring-cloud-alibaba-dependencies</artifactId>
								<version>2.1.0.RELEASE</version>
								<type>pom</type>
								<scope>import</scope>
							</dependency>
						</dependencies>
					</dependencyManagement>
				b. 本模块中的POM
					<dependency>
						<groupId>com.alibaba.cloud</groupId>
						<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
					</dependency>
			C. 写YML：
				server.port=9001
				spring.application.name=nacos-payment-provider
				spring.cloud.nacos.discovery.server-addr=192.168.107.6:8848
				management.endpoints.web.exposure.include=*
			D. 主启动
				@SpringBootApplication
				@EnableDiscoveryClient
				public class NacosPaymentMain9001 {
					public static void main(String[] args) {
						SpringApplication.run(NacosPaymentMain9001.class, args);
					}
				}
			E. 如果不想重复新建工程，可以拷贝虚拟端口映射
				a. 在Run Dashboard中选择要拷贝的工程，右键 -> 选择copy configuration
				b. 修改Name输入框的内容，在VM options模块中输入端口信息，-DServer.port=9002
				c. 点击Apply和OK完成设置并关闭页面。
			F. 业务类
				@RestController
				public class PaymentController {

					@Value("${server.port}")
					private String serverPort;

					@GetMapping("/payment/nacos/{id}")
					public String getPayment(@PathVariable("id") Integer id) {
						return "nacos registry, serverPort: " + serverPort + ", id" + id;
					}
				}
			G. 测试：在浏览器地址栏输入地址：http://localhost:9001/payment/nacos/1和http://localhost:9002/payment/nacos/1
		③ 基于Nacos的服务消费者
			A. 新建Module，cloudalibaba-consumer-order80
			B. 改POM，参照cloudalibaba-provider-payment9001的POM文件
			C. 写YML：
				server:
				  port: 80

				spring:
				  application:
					name: nacos-order-consumer
				  cloud:
					nacos:
					  discovery:
						server-addr: 192.168.107.6:8848

				# 消费者将要去访问的微服务名称（成功注册进Nacos的微服务提供者）
				service-url:
				  nacos-user-service: http://nacos-payment-provider
			D. 主启动：OrderNacosMain80
			E. 业务类：
				@Configuration
				public class ApplicationContextConfig {

					@Bean
					@LoadBalanced
					public RestTemplate restTemplate() {
						return new RestTemplate();
					}
				}
				@RestController
				public class OrderNacosController {

					@Autowired
					private RestTemplate restTemplate;

					@Value("${service-url.nacos-user-service}")
					private String serviceUrl;

					@GetMapping("/consumer/payment/nacos/{id}")
					public String payemntInfo(@PathVariable("id") Integer id) {
						return restTemplate.getForObject(serviceUrl + "/payment/nacos/" + id, String.class);
					}
				}
			F. 启动并测试服务消费者：http://localhost/consumer/payment/nacos/1
		④ 服务注册中心比较
			A. Nacos和CAP
				-------------------------------------------------------------------------------------------------------------
				|			   	  | Nacos 					  	| Eureka 		| Consul 			| CoreDNS | Zookeeper 	|
				| 一致性协议   	  | CP/AP 					  	| AP 	  		| CP 	   			| / 	  | CP 			|
				| 健康检查    	  | TCP/HTTP/MySQL/Client Beat  | Client Beat 	| TCP/Http/gRPC/Cmd | /		  | Client Beat |
				| 负载均衡	   	  | 权重/DSL/metadata/CMDB	  	| Ribbon		| Fabio				| RR	  | /			|
				| 雪崩保护	   	  | 支持						| 支持			| 不支持			| 不支持  | 不支持 		|
				| 自动注销实例 	  | 支持						| 支持			| 不支持			| 不支持  | 支持		|
				| 访问协议	   	  | HTTP/DNS/UDP				| HTTP 			| HTTP/DNS			| DNS/UDP | TCP			|
				| 监听支持	   	  | 支持						| 支持			| 支持				| 不支持  | 支持		|
				| 多数据中心   	  | 支持						| 支持			| 支持				| 不支持  | 不支持		|
				| 跨注册中心   	  | 支持						| 不支持		| 支持				| 不支持  | 不支持		|
				| SpringCloud集成 | 支持						| 支持 			| 支持				| 不支持  | 不支持		|
				| Dubbo集成		  | 支持						| 不支持		| 不支持			| 不支持  | 支持		|
				| K8s 集成		  | 支持						| 不支持		| 支持				| 支持	  | 不支持		|
				-------------------------------------------------------------------------------------------------------------
			B. 切换Nacos的一致性协议
				a. Nacos支持AP和CP模式的切换
				b. C是所有节点在同一时间看到的数据是一致的，而A的定义是所有的请求都会收到响应
				c. 何时选择使用何种模式?
					(1)一般来说，如果不需要存储服务级别的信息且服务实例是通过Nacos-client注册的，并能够保持心跳上报，那么那么就可以选择AP模式。当前主流的服务如Spring 
					Cloud和Dubbo服务，都适用于AP模式。AP模式为了服务的可用性而减弱了一致性，因此AP模式下只支持注册临时实例
					(2) 如果需要在服务级别编辑或者存储配置信息，那么CP是必须的，K8s服务和DNS服务则适用于CP模式。CP模式下则注册持久化实例，此时则是以Raft协议为集群运行
					模式，该模式注册实例之前必须先注册服务，如果服务不存在，则会返回错误
				d. 如何切换AP和CP模式：curl -X PUT '$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP'
	4. Nacos作为服务配置中心演示
		① Nacos作为配置中心-基础配置
			A. 新建Module，cloudalibaba-config-nacos-client3377
			B. 改POM，引入spring-cloud-starter-alibaba-nacos-config和spring-cloud-starter-alibaba-nacos-discovery依赖
				<dependency>
					<groupId>com.alibaba.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-nacos-config</artifactId>
				</dependency>
				<dependency>
					<groupId>com.alibaba.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
				</dependency>
			C. 写YML
				a. 需要配置两个主配置文件，bootstrap.yml和application.yml
				b. 为什么需要使用两个YML配置文件
					(1) Nacos同SpringCloud Config一样，在项目初始化时，要保证先从配置中心进行配置拉取，拉取配置之后，才能保证目标服务正常启动
					(2) SpringBoot中配置文件的加载是存在优先级顺序的，bootstrap优先级高于application
				c. bootstrap.yml
					server:
					  port: 3377

					spring:
					  application:
						name: nacos-config-client
					  cloud:
						nacos:
						  discovery:
							server-addr: 192.168.107.6:8848 # Nacos服务注册中心地址
						  config:
							server-addr: 192.168.107.6:8848 # Nacos作为配置中心地址
							file-extension: yaml # 指定yaml格式的配置
				d. application.yml
					spring:
					  profiles:
						active: dev #激活开发环境
			D. 主启动：使用@EnableDiscoveryClient注解标注
			E. 业务类：通过Spring Cloud 原生注解@RefreshScope实现配置自动更新
				@RestController
				@RefreshScope // 支持Nacos的动态刷新
				public class ConfigClientController {

					@Value("${config.info}")
					private String configInfo;

					@GetMapping("/config/info")
					public String getConfigInfo() {
						return configInfo;
					}
				}
			F. 在Nacos中添加配置信息，Nacos中匹配规则
				a. 理论：Nacos的dataid的组成格式及与SpringBoot配置文件中的配置规则
					(1) 官网：https://nacos.io/zh-cn/docs/quick-start-spring-cloud.html
					(2) 说明：之所以需要配置spring.application.name，是因为它构成了Nacos配置管理的一部分。
					(3) 在Nacos Spring Cloud中，data id的完整格式为：${prefix}-${spring.proflies.active}.${file-extension}
						(A) prefix：默认是spring.application.name的值，也可以通过配置项spring.cloud.nacos.config.prefix来配置
						(B) spring.proflix.active：默认为当前环境对应的profile，详情可以参考Spring Boot文档，注意：当spring.profiles.active为空时，对应的连接符“-”也将不存在，
						data id的拼接格式变为${prefix}.${file-extension}
						(C) ${file-extension}：为配置内容的数据格式，可以通过配置项：spring.cloud.nacos.config.file-extension来配置，目前只支持properties和yaml类型。
						(D) 通过Spring Cloud原生注解@RefreshScope实现配置自动更新
						(E) 最终公式为：${spring.application.name}-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension}，即nacos-config-client-dev.yaml
				b. 实操：在Nacos的主界面中选择 配置管理 -> 配置列表，点击右侧的“+”，在Data Id中按照公式输入与工程相对应的文件名nacos-config-client-dev.yaml，并在配置格式中选择yaml
				在配置内容窗体中输入内容，点击发布完成配置。
			G. 测试：
				a. 启动前需要早Nacos客户端->配置管理->配置管理栏目下有对应的yaml配置文件
				b. 运行cloudalibaba-config-nacos-client3377的主启动类
				c. 调用接口查看配置信息
			H. 自带动态刷新：修改Nacos中的yaml配置文件，再次调用查看配置的接口，就会发现配置已经被刷新。
		② Nacos作为配置中心-分类配置
			A. 问题：多环境多项目管理
				a. 问题一：实际开发中，通常一个系统会准备dev开发环境、test测试环境和prod生产环境，如何保证指定环境启动时服务能正确读取到Nacos上相应环境的配置文件呢？
				b. 问题二：一个大型分布式微服务系统会有很多微服务子项目，每天微服务项目又都会相应的开发环境、测试环境、正式环境……那怎么对这些微服务配置进行管理？
			B. Namespace+Group+Data ID三者关系？为什么这么设计
				a. 类似于Java里面的package名和类名，最外层的namespace是可以用于区分部署环境的，Group和DataID逻辑上区分两个目标对象
				b. 默认情况：Namespace=public，Group=DEFAULT_GROUP，默认Cluster是DEFAULT
				c. Nacos默认的命名空间是public，Namespace主要用来实现隔离，比方说有三个环境：开发、测试、生产环境，可以创建三个Namespace，不同的Namespace之间是隔离的。
				d. Group默认是DEFAULT_GROUP，Group可以把不同微服务划分到同一个分组里面去
				f. Service就是微服务，一个Service可以包含多个Cluster（集群），Nacos默认Cluster，Cluster是对指定微服务的一个虚拟划分，比方说为了容灾，将Service微服务分别部署在了杭州
				机房和广州机房，这时就可以给杭州机房的Service微服务起一个集群名称(HZ)，给广州机房的Service微服务起一个集群名称（GZ），还可以尽量让同一个机房的微服务互相调用，以提升性能。
				g. 最后是instance，就是微服务的实例。
			C. 三种方案加载配置
				a. Data ID方案：
					(1) 指定spring.profiles.active和配置文件的Data ID来使不同环境下读取不同的配置
					(2) 默认空间+默认分组+新建dev和test两个Data ID，nacos-config-client-dev.yaml和nacos-config-client-test.yaml
					(3) 通过spring.profiles.active属性就能进行多环境下配置文件的读取
				b. Group方案：
					(1) 通过Group实现环境分区，新建Group，DEV_GROUP和TEST_GROUP
					(2) 在Nacos中新建配置文件Data ID，nacos-config-client-info.yaml
					(3) 修改bootstrap和application的YML配置
						(A) 在bootstrap中添加spring.cloud.nacos.config.group=DEV_GROUP属性
						(B) 在application中切换环境为info
						(C) 测试：通过spring.cloud.nacos.config.group属性切换分组，可以获取不同分组的配置信息
				c. Namespace方案：
					(1) 新建dev/test的Namespace
					(2) 在dev的Namespace中新建DEFAULT_GROUP、DEV_GROUP和TEST_GROUP分组和新建Data ID：nacos-config-client-info.yaml
					(3) 在bootstrap中添加spring.cloud.nacos.config.namespace=661cfa42-2fe2-4395-9f08-4636bbb884e1属性，指定namespace环境
					(4) 测试：切换namespace，通过spring.cloud.nacos.config.group属性切换分组，可以获取不同分组的配置信息
	5. Nacos集群和持久化配置（重要）
		① 官网说明
			A. 官网地址：https://nacos.io/zh-cn/cluster-mode-quick-start.html
			B. 官网架构图
										-------------
										| nacos.com |
										-------------
											  ↓
								   ----------------------
								   | VIP（Nginx虚拟IP） |
								   ----------------------
						----------------------|-----------------------
						↓					  ↓						 ↓
				---------------			---------------		   ---------------
				| Nacos（ip1）|			| Nacos（ip2）|		   | Nacos（ip3）|
				---------------			---------------		   ---------------
						|					  |						 |
						----------------------------------------------
											  ↓
										-------------
										| MySQL集群 |
										-------------
		② Nacos持久化配置解释
			A. Nacos默认自带的是嵌入式数据库Derby，https://github.com/alibaba/nacos/blob/develop/config/pom.xml
				<dependency>
					<groupId>org.apache.derby</groupId>
					<artifactId>derby</artifactId>
				</dependency>
			B. Derby到mysql切换配置步骤
				a. 在nacos\conf目录下找到SQL脚本nacos-mysql.sql，新建nacos_config数据库，并执行脚本
				b. 在nacos\conf目录下找到SQL脚本application.properties，增加以下配置
				c. 如果是使用docker容器启动nacos，则需要进入容器，修改conf/application.properties，增加连接数据库信息
					(1) docker exec -it <容器ID> bash
					(2) vim conf/application.properties 
					(3) 配置内容：
						spring.datasource.platform=mysql
						
						db.num=1
						db.url.0=jdbc:mysql://192.168.107.6:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true
						db.user=root
						db.password=2648
					(4) 重启Nacos容器
			C. 启动Nacos，可以看到是个全新的空记录界面，以前是记录进Derby
		③ Linux版Nacos+MySQL生产环境配置
			A. 预计需要1个Nginx+3个nacos注册中心+1个mysql
			B. Nacos下载Linux版本
			C. 集群配置步骤（重点）：
				a. Linux服务器上mysql数据库的配置，执行nacos\conf\nacos-mysql.sql的SQL脚本
				b. 在nacos\conf\application.properties中配置mysql信息
				c. Linux服务器上nacos的集群配置cluster.conf
					(1) 如果只存在cluster.conf.example，则复制cluster.conf.example到cluster.conf
						cp cluster.conf.example cluster.conf
					(2) 梳理出3台nacos集群
					(3) 内容
						192.168.107.6:8848
						192.168.107.8:8848
						192.168.107.29:8848
				d. 编辑Nacos的启动脚本nacos/bin/startup.sh，使它能够接受不同的启动端口
					/*
					(1) 思考：
						(A) 单机版的启动，都是执行nacos/bin/startup.sh命令即可，但是，集群启动，希望可以类似其他软件的shell命令，传递不同的端口号启动不同的nacos实例。
						(B) 命令：./startup.sh -p 8848表示启动端口号8848的nacos服务器实例，和上一步cluster.conf配置的一致。
					(2) 修改内容
						(A) 修改前：
							while getopts ":m:f:s:" opt
							do
								case $opt in
									m)
										MODE=$OPTARG;;
									f)
										FUNCTION_MODE=$OPTARG;;
									s)
										SERVER=$OPTARG;;
									?)
									echo "Unknown parameter"
									exit 1;;
								esac
							done
							...
							# start
							echo "$JAVA ${JAVA_OPT}" > ${BASE_DIR}/logs/start.out 2>&1 &
							nohup $JAVA ${JAVA_OPT} nacos.nacos >> ${BASE_DIR}/logs/start.out 2>&1 &
							echo "nacos is starting，you can check the ${BASE_DIR}/logs/start.out"
						(B) 修改后：
							while getopts ":m:f:s:p:" opt
							do
								case $opt in
									m)
										MODE=$OPTARG;;
									f)      
										FUNCTION_MODE=$OPTARG;;
									s)      
										SERVER=$OPTARG;;
									p)      
										PORT=$OPTARG;;
									?)      
									echo "Unknown parameter"
									exit 1;;
								esac    
							done
							...
							# start
							echo "$JAVA ${JAVA_OPT}" > ${BASE_DIR}/logs/start.out 2>&1 &
							nohup $JAVA -Dserver.port=${PORT} ${JAVA_OPT} nacos.nacos >> ${BASE_DIR}/logs/start.out 2>&1 &
							echo "nacos is starting，you can check the ${BASE_DIR}/logs/start.out"
						*/
						(1) 修改前：
							#===========================================================================================
							# JVM Configuration
							#===========================================================================================
							if [[ "${MODE}" == "standalone" ]]; then
								JAVA_OPT="${JAVA_OPT} -Xms512m -Xmx512m -Xmn256m"
								JAVA_OPT="${JAVA_OPT} -Dnacos.standalone=true"
							else
								JAVA_OPT="${JAVA_OPT} -server -Xms2g -Xmx2g -Xmn1g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"
								JAVA_OPT="${JAVA_OPT} -XX:-OmitStackTraceInFastThrow -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${BASE_DIR}/logs/java_heapdump.hprof"
								JAVA_OPT="${JAVA_OPT} -XX:-UseLargePages"
							fi
						(2) 修改后：
							#===========================================================================================
							# JVM Configuration
							#===========================================================================================
							if [[ "${MODE}" == "standalone" ]]; then
								JAVA_OPT="${JAVA_OPT} -Xms512m -Xmx512m -Xmn256m"
								JAVA_OPT="${JAVA_OPT} -Dnacos.standalone=true"
							else
								JAVA_OPT="${JAVA_OPT} -server -Xms2g -Xmx2g -Xmn1g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"
								JAVA_OPT="${JAVA_OPT} -XX:-OmitStackTraceInFastThrow -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${BASE_DIR}/logs/java_heapdump.hprof"
								JAVA_OPT="${JAVA_OPT} -XX:-UseLargePages"
								JAVA_OPT="${JAVA_OPT} -Dnacos.server.ip=192.168.107.8"
							fi
				e. Nginx的配置，由它作负载均衡器
					#gzip  on;
					upstream cluster{
						server 192.168.107.6:8048;
						server 192.168.107.8:8048;
						server 192.168.107.29:8048;
					}
					
					server {
						listen       1111;
						server_name  192.168.107.6;
						
						location / {
							#root   html;
							#index  index.html index.htm;
							proxy_pass http://cluster;
						}
				f. 按照指定启动
					(1) 启动mysql
					(2) 启动nacos集群
						./startup.sh -p 8826，./startup.sh -p 8848，./startup.sh -p 8855
					(3) 启动Nginx
				g. 截止到此，一个Nginx+三个nacos+一个mysql搭建完毕
					(1) 测试通过Nginx访问nacos：http://192.168.107.6:1111/nacos
					(2) 新建一个配置测试：
					(3) 修改cloudalibaba-config-nacos-client3377的bootstrap配置文件，修改Nacos服务注册中心地址为nginx的地址
					(4) 启动cloudalibaba-config-nacos-client3377并访问地址http://localhost:3377/config/info，成功获取nacos集群的配置文件
十九、Spring Cloud Alibaba Sentinel实现熔断与限流
	1. Sentinel：
		① 官网：
			A. 英文文档：https://github.com/alibaba/Sentinel
			B. 中文文档：m/alibaba/Sentinel/wiki/介绍
		② 介绍：
			A. 随着微服务的流行，服务和服务之间的稳定性变得越来越重要。Sentinel 以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。
			B. Sentinel类似于Hystrix
		③ 下载地址：https://github.com/alibaba/Sentinel/releases
		④ 使用：
			A. 官网：https://spring-cloud-alibaba-group.github.io/github-pages/greenwich/spring-cloud-alibaba.html#_spring_cloud_alibaba_sentinel
			B. 服务使用中的各种问题
				a. 服务雪崩
				b. 服务降级
				c. 服务熔断
				d. 服务限流
	2. 安装Sentinel控制台：
		① sentinel组件由两部分构成：
			A. 后台，核心库（Java客户端）：不依赖任何框架/库，能够允许于所有Java运行环境，同时对Dubbo/Spring Cloud等框架也有较好的支持
			B. 前台8080，控制台（Dashboard）：基于Spring Boot开发，打包后可以直接运行，不需要额外的Tomcat等应用容器。
		② 安装步骤
			A. 下载：
				a. 地址：https://github.com/alibaba/Sentinel/releases
				b. 版本：sentinel-dashboard-1.7.0.jar
			B. 运行命令：
				a. 前提：java8环境和8080端口不能被占用
				b. 命令：使用8089端口运行，java -jar sentinel-dashboard-1.7.0.jar --server.port=8089
			C. 访问sentinel管理界面：http://localhost:8089/，默认账号和密码为sentinel
	3. 初始化演示工程
		① 启动Nacos8848成功，用作服务注册中心
		② 新建Module
			A. 新建cloudalibaba-sentinel-service8401
			B. 改POM
				<dependency>
					<groupId>com.alibaba.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
				</dependency>
				<dependency>
					<groupId>com.alibaba.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
				</dependency>
				<dependency>
					<groupId>com.alibaba.csp</groupId>
					<artifactId>sentinel-datasource-nacos</artifactId>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-openfeign</artifactId>
				</dependency>
			C. 写YML
				server:
				  port: 8401

				spring:
				  application:
					name: cloudalibaba-sentinel-service
				  cloud:
					nacos:
					  discovery:
						server-addr: 192.168.107.6:1111
					sentinel:
					  transport:
						dashboard: localhost:8089
						port: 8719

				management:
				  endpoints:
					web:
					  exposure:
						include: '*'
			D. 主启动：SentinelMain8401
			E. 业务类：FlowLimitController
		③ 启动Sentinel8089：java -jar sentinel-dashboard-1.7.0.jar --server.port=8089
		④ 启动微服务8401
		⑤ 启动8401微服务后查看sentinel控制台：
			A. Sentinel采用懒加载机制，需要访问一次http://localhost:8401/testA和http://localhost:8401/testB即可
			B. 结论：Sentinel8089正在监控微服务8401
	4. 流控规则
		① 基本介绍
			A. 资源名：唯一名称，默认请求路径
			B. 针对来源：Sentinel可以针对调用者进行限流，填写微服务名，默认default（不区分来源）
			C. 阈值类型单机阈值：
				a. QPS（每秒的请求数量）：当调用该API的QPS达到阈值的时候，进行限流
				b. 线程数：当调用该API的线程数达到阈值的时候，进行限流
			D. 是否集群：不需要集群
			E. 流控模式：
				a. 直接：API达到限流条件时，直接限流
				b. 关联：当关联的资源达到阈值时，就限制自己
				c. 链路：只记录指定链路上的流量（指定资源从入口资源进来的流量，如果达到阈值，就进行限流），API级别的针对来源
			F. 流控效果：
				a. 快速失败：直接失败，抛异常
				b. Warm Up：根据codeFactor（冷加载因子，默认3）的值，从阈值 / codeFactor，经过预热时间，才达到设置的QPS阈值
				c. 排队等待：匀速排队，让请求以均速的速度通过，阈值类型必须设置为QPS，否则无效
		② 流控模式
			A. 直接（系统默认）
				a. 添加入口：簇点链路和流控规则
				b. 添加流控规则-QPS
					(1) 选择资源名称，针对来源，阈值类型，单机阈值，是否集群
					(2) 在高级选项中的流控模式中选择直接，在流控效果中选择直接失败
					(3) 点击新增完成流控规则的添加
				c. 访问地址http://localhost:8401/testA进行测试，如果超出阈值则报Blocked by Sentinel (flow limiting)异常
				d. 添加流控规则-线程数，选择阈值类型为线程数，该效果与QPS不同
			B. 关联
				a. 当与A关联的资源B达到阈值后，就限流A自己
				b. 设置效果，当关联资源/testB的QPS阈值超过1时，就限流/testA的Rest访问，即当关联资源到阈值后限制配置好的资源名
				c. 使用postman模拟并发密集访问testB
					(1) 在测试地址栏中输入地址http://localhost:8401/testB，选择请求方式为GET，点击Send，自测通过
					(2) 在左侧新建collection集合Collection2020
					(3) 点击save保存测试实例到新建的Collection2020集合
					(4) 选择新建的Collection2020的箭头图标打开，选择保存的http请求实例，点击run进入循环测试页面
					(5) 在Iterations中输入20，在Delay中输入300，表示每隔300ms，发送一个线程，点击Run Collection2020
					(6) 此时/testB已经达到阈值了，再次访问/testA时，访问/testA时报Blocked by Sentinel (flow limiting)异常
			C. 链路：多个请求调用同一个微服务
		③ 流控效果
			A. 直接->快速失败
				a. 直接失败，抛出异常：Blocked by Sentinel (flow limiting)
				b. 源码：com.alibaba.csp.sentinel.slots.block.flow.controller.DefaultController
			B. 预热：
				a. 公式：开始值等于阈值除以coldFactor（冷因子默认值为3），经过预热时长后才会达到阈值
				b. 官网：
					(1) https://github.com/alibaba/Sentinel/wiki/流量控制，Warm Up（RuleConstant.CONTROL_BEHAVIOR_WARM_UP）方式，即预热/冷启动方式。当系统长期处于低水位的情况下，当流量
					突然增加时，直接把系统拉升到高水位可能瞬间把系统压垮。通过"冷启动"，让通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，给冷系统一个预热的时间，避免冷系统被压垮。
					(2) 解释：默认coldFactor为3，即请求QPS从threshold / 3开始，经预热时长逐渐升至设定的QPS阈值。
					(3) 限流冷启动：https://github.com/alibaba/Sentinel/wiki/%E9%99%90%E6%B5%81---%E5%86%B7%E5%90%AF%E5%8A%A8
				c. 源码：com.alibaba.csp.sentinel.slots.block.flow.controller.WarmUpController
				d. WarmUp配置：
					a. 案例：阈值为10+预热时长设置为5秒
					b. 系统初始值的阈值为10 / 3约等于3，即阈值刚开始为3，然后经过5秒后阈值才慢慢升高恢复到10
				e. 多次访问	http://localhost:8401/testB，刚开始不行，后来慢慢就可以了
				f. 应用场景，如：秒杀系统在开启的瞬间，会有很多流量上来，很多情况会把系统打死，预热方式就是为了保护系统，可慢慢的把流量放进来，慢慢的把阈值增长到设置的阈值
			C. 排队等待：
				a. 匀速排队
					(1) 匀速排队（RuleConstant.CONTROL_BEHAVIOR_RATE_LIMITER）方式会严格控制请求通过的间隔时间，也即是让请求以均匀的速度通过，对应的是漏桶算法。
					(2) 这种方式主要用于处理间隔性突发的流量，例如消息队列。想象一下这样的场景，在某一秒有大量的请求到来，而接下来的几秒则处于空闲状态，我们希望系统能够在接下来的空闲期间
					逐渐处理这些请求，而不是在第一秒直接拒绝多余的请求。
					(3) 让请求以均匀的速度通过，阈值类型必须设置为QPS，否则无效。
				b. 设置含义：/testA每秒1次请求，超过的话就排队等待，等待的超时时间为20000ms
				c. 官网：https://github.com/alibaba/Sentinel/wiki/%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6
				d. 源码：com.alibaba.csp.sentinel.slots.block.flow.controller.RateLimiterController
				f. 测试：使用postman进行测试，使用collection模式进行访问，设置iterations为10个线程，设置delay为1000ms，表示发送10个线程，每隔1秒发送一个线程。
	5. 降级规则
		① 官网：https://github.com/alibaba/Sentinel/wiki/%E7%86%94%E6%96%AD%E9%99%8D%E7%BA%A7
		② 基本介绍：
			A. 平均响应时间 (DEGRADE_GRADE_RT)：当 1s 内持续进入 N 个请求，对应时刻的平均响应时间（秒级）均超过阈值（count，以 ms 为单位），那么在接下的时间窗口（DegradeRule 中的 timeWindow，
			以 s 为单位）之内，对这个方法的调用都会自动地熔断（抛出 DegradeException）。注意 Sentinel 默认统计的 RT 上限是 4900 ms，超出此阈值的都会算作 4900 ms，若需要变更此上限可以通过启动
			配置项 -Dcsp.sentinel.statistic.max.rt=xxx 来配置。
			B. 异常比例 (DEGRADE_GRADE_EXCEPTION_RATIO)：当资源的每秒请求量 >= N（可配置），并且每秒异常总数占通过量的比值超过阈值（DegradeRule 中的 count）之后，资源进入降级状态，即在接下的
			时间窗口（DegradeRule 中的 timeWindow，以 s 为单位）之内，对这个方法的调用都会自动地返回。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。
			C. 异常数 (DEGRADE_GRADE_EXCEPTION_COUNT)：当资源近 1 分钟的异常数目超过阈值之后会进行熔断。注意由于统计时间窗口是分钟级别的，若 timeWindow 小于 60s，则结束熔断状态后仍可能再进入熔断状态。
			D. 进一步说明，Sentinel熔断降级会发生在调用链路中某个资源出现不稳定状态时（例如：调用超时或者异常比例升高），对这个资源的调用进行限制，让请求快速失败，避免影响到其他资源而导致级联错误。
			当资源被降级后，在接下来的降级时间窗口内，对该资源的调用都会自动熔断（默认行为是抛出DegradeException）
			E. Sentinel的断路器是没有半开状态的，半开的状态系统自动检测是否请求有异常，没有异常就关闭断路器恢复使用，有异常则就继续打开断路器不可用。
		③ 降级策略
			A. RT
				a. 介绍
				------------------------
				| 秒钟持续进入5个请求  |					------------------
				| 平均响应时间大于阈值 |					|	关闭降级	 |
				------------------------					------------------
							|										↑
							↓										|
				------------------------							|
				| 		触发降级	   |					------------------
				|	 （断路器打开）	   | -----------------> | 时间窗口期结束 |
				------------------------					------------------
				b. 测试
					(1) 代码
						@GetMapping("/testB")
						public String testB() {
							try {
								Thread.sleep(1000);
							} catch (InterruptedException e) {
								e.printStackTrace();
							}
							log.info("testB，测试RT");
							return "testB";
						}
					(2) 配置Sentinel的熔断降级，新增降级规则，输入资源名/testB，选择降级策略为RT，RT为500毫秒，时间窗口期为5秒。
					(3) 使用JMeter进行测试，每一秒钟请求十个线程满足服务降级的条件，启动测试后，在浏览器访问时，断路器打开，访问出异常，当停止JMeter测试后，5秒钟的窗口期过后，请求恢复正常
			B. 异常比例 
				a. 介绍：
					------------------------
					| QPS>=5 并且异常比例  |					------------------
					| （秒级统计）超过阈值 |					|	关闭降级	 |
					------------------------					------------------
								|										↑
								↓										|
					------------------------							|
					| 		触发降级	   |					------------------
					|	 （断路器打开）	   | -----------------> | 时间窗口期结束 |
					------------------------					------------------
				b. 测试
					(1) 代码：
						@GetMapping("/testB")
						public String testB() {
							log.info("testB，测试RT");
							int i = 1 / 0;
							return "testB";
						}
					(2) 配置Sentinel的降级策略，修改/testB降级规则，降级策略改为异常比例，异常比例数为0.2，时间窗口为5秒。
					(3) 使用JMeter测试/testB，此时在浏览器访问/testB，断路器已经被打开，该服务已经被熔断降级，当停止JMeter测试后，5秒钟的窗口期过后，由于触发熔断降级的条件不满足，因此
					发送请求时恢复为原来的错误页面
			C. 异常数
				a. 介绍：
					------------------------
					| 异常数（分钟统计）   |					------------------
					| 		超过阈值 	   |					|	关闭降级	 |
					------------------------					------------------
								|										↑
								↓										|
					------------------------							|
					| 		触发降级	   |					------------------
					|	 （断路器打开）	   | -----------------> | 时间窗口期结束 |
					------------------------					------------------
				b. 异常数是按照分钟统计的，时间窗口一定要大于等于60秒
				c. 测试：
					(1) 配置Sentinel的降级策略，修改/testB降级规则，降级策略改为异常数，异常数为1，时间窗口为60秒
					(2) 在浏览器访问/testB，前五次访问会出现error页面，因为没达到一分钟5次异常数的条件，到达5次报错之后，进入熔断降级
	7. 热点key限流				
		① 基本介绍： 
			A. 何为热点？热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数据，并对其访问进行限制。比如：
				商品 ID 为参数，统计一段时间内最常购买的商品 ID 并进行限制
				用户 ID 为参数，针对一段时间内频繁访问的用户 ID 进行限制
			B. 热点参数限流会统计传入参数中的热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流。热点参数限流可以看做是一种特殊的流量控制，仅对包含热点参数的资源调用生效。
				Sentinel Parameter Flow Control
			C. Sentinel 利用 LRU 策略统计最近最常访问的热点参数，结合令牌桶算法来进行参数级别的流控。热点参数限流支持集群模式。
		② 官网：https://github.com/alibaba/Sentinel/wiki/热点参数限流
		③ 承上启下复习start
			A. 服务降级方法分为系统默认和客户自定义两种之前的案例都是限流出问题后，都是用Sentinel系统默认的提示：Blocked by Sentinel (flow limiting)
			B. 自定义降级方法，类似于Hystrix
			C. 结论：从HystrixCommand到@SentinelResource
		④ 代码：com.alibaba.csp.sentinel.slots.block.BlockException
			@GetMapping("/testHotKey")
			@SentinelResource(value = "testHotKey", blockHandler = "deal_hotKey")
			public String testHotKey(@RequestParam(value = "p1", required = false) String p1,
									  @RequestParam(value = "p2", required = false) String p2,) {
				return "testHotKey";
			}
			public String deal_hotKey(String p1, String p2, BlockException e) {
				return "deal_hotKey";
			}
		⑤ 配置：对testHotKey添加热点规则，限流模式QPS模式，参数索引为0，单机阈值为1， 统计窗口时长为1秒。
		⑥ 测试
			A. 如果配置了@SentinelResource(value = "testHotKey")，不指定blockHandler服务降级方法，访问页面会出现异常的页面
			B. 配置了@SentinelResource(value = "testHotKey", blockHandler = "deal_hotKey")服务降级方法之后，如果第一个参数只要QPS超过每秒1次，马上执行自定义的降级方法
			C. 建议配置了热点规则之后，使用blockHandler属性指定自定义的服务降级方法。
			D. 测试实例：
				a. http://localhost:8401/testHotKey?p2=p2：成功
				b. http://localhost:8401/testHotKey?p1=p1：失败
				c. http://localhost:8401/testHotKey?p1=p1&p2=p2：失败
		⑦ 参数例外项
			A. 上述案例演示了第一个参数p1，当QPS超过1秒1次点击后马上被限流
			B. 特例情况
				a. 普通情况：超过1秒1次访问时，达到阈值1后马上就被限流了
				b. 期望p1参数是某个特殊值时，它的限流值和平时不一样
				c. 特例：假如p1的值为LiXL时，它的阈值可以达到200
			C. 配置：选择参数类型为String，参数值为LiXL，限流阈值为200，点击添加完成一次添加，可以添加多个参数例外项
			D. 测试：
				a. http://localhost:8401/testHotKey?p1=LiLX：p1等于LiLX时，QPS为200，访问成功
				b. http://localhost:8401/testHotKey?p1=p1：p1不等于LiLX或者LiXL时，QPS为1，超过阈值访问失败
			E. 前提条件：热点参数的注意点，参数必须是基本类型或者引用类型String
		⑧ 其他：手动添加异常时，不会引起服务服务熔断降级，而是直接报异常，因为@SentinelResource是负责Sentinel控制台的服务限流规则从而引发服务降级引用自定义的服务降级方法，而无法处理运行时异常
	8. Sentinel系统规则：
		① 基本介绍
			A. 官网：https://github.com/alibaba/Sentinel/wiki/系统自适应限流
			B. Sentinel系统自适应限流从整体维度对应用入口流量进行控制，结合应用的Load、CPU使用率、总体平均RT、入口QPS和并发线程数等几个维度的监控指标，通过自适应的流控策略，让系统的入口流量和
			系统的负载达到一个平衡，让系统尽可能跑在最大吞吐的同时保证系统整体的稳定性
		② 各项配置参数说明
			A. Load自适应（仅对Linux/Unix-like机器生效）：系统的load1作为启发指标，进行自适应系统保护，当系统load1超过设定的启发值，且系统当前的并发线程数超过估算的系统容量时才会触发保护（BBR阶段）。
			系统容量有系统的maxQps * minRT估算得出，设定参数值一般是CPU cores * 2.5。
			B. CPU usage（1.5.0+ 版本）：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0），比较灵敏。
			C. 平均 RT：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。
			D. 并发线程数：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。
			E. 入口 QPS：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。
		③ 配置全局QPS：新增系统规则，选择阈值类型为入口QPS，阈值为1，表示微服务所有请求的每秒次数不能超过1
		④ 不建议使用系统规则，该配置针对的是整体的设置，没有细粒度
	9. @SentinelResource
		① 按资源名称限流+后续处理
			A. 启动Nacos和Sentinel
			B. 修改cloudalibaba-sentinel-service8401，增加业务类
				@GetMapping("/byResource")
				@SentinelResource(value = "byResource", blockHandler = "handleException")
				public CommonResult byResource() {
					return new CommonResult(200, "按资源名称限流OK", new Payment(2020L, "940906"));
				}
				public CommonResult handleException(BlockException e) {
					return new CommonResult(500, e.getClass().getCanonicalName() + ", 服务不可用");
				}
			C. 配置流控规则，按照资源名称，即@SentinelResource注解的value属性值进行流控规则配置，设置阈值为1
			D. 测试，如果每秒钟访问一次，即可正常访问，超过则会调用自定义的限流处理。
			E. 额外问题：关闭服务8401，Sentinel控制台，流控规则消失，说明流控规则配置是临时的
		② 按照URL地址限流+后续处理
			A. 业务类：
				@GetMapping("/byUrl")
				@SentinelResource(value = "byUrl")
				public  CommonResult byUrl() {
					return new CommonResult(200, "按Url限流OK",  new Payment(2L, "921127"));
				}
			B. 配置流控规则，按照URL地址，即@GetMapping的属性，设置阈值为1
			C. 测试，如果每秒钟访问一次，即可正常访问，超过则会调用默认的Sentinel限流处理。
		③ 上面服务降级方案面临的问题
			A. 系统默认的，没有体现自己的业务要求
			B. 依照现有条件，自定义的处理方法和业务代码耦合，不直观
			C. 每一个方法都添加一个降级方法，造成代码膨胀加剧
			D. 全局统一的处理方法没有体现
		④ 客户自定义限流处理逻辑
			A. 创建CustomerBlockHandler类用于自定义限流处理逻辑
			B. 自定义限流处理
				public class CustomerBlockHandler {
					public static CommonResult handlerException(BlockException e) {
						return new CommonResult(404, "按客户自定义，global handlerException");
					}
					public static CommonResult handlerException2(BlockException e) {
						return new CommonResult(404, "按客户自定义，global handlerException2");
					}
				}
			C. 修改业务类
				@GetMapping("customerBlockHandler")
				@SentinelResource(value = "customerBlockHandler",
						blockHandlerClass = CustomerBlockHandler.class,
						blockHandler = "handlerException2")
				public CommonResult customerBlockHandler() {
					return new CommonResult(200, "按客户自定义", new Payment(3L, "960730"));
				}
			D. Sentinel控制台配置：按照资源名称进行流控，设置单击阈值为1的QPS，
			E. 测试后自定义的服务降级方法被调用
		⑤ 更多注解属性说明
			A. @SentinelResource注解的所有代码都要使用try-catch-finally方式进行处理
			B. Sentinel主要有三个核心API
				a. SphU定义资源
				b. Tracer定义统计
				c. ContextUtil定义了上下文
	10. 服务熔断功能
		① sentinel整合ribbon+OpenFeign+fallback
		② Ribbon系列
			A. 启动nacos和sentinel
			B. 提供者9003/9004
				a. 新建cloudalibaba-provider-payment9003/9004
				b. 改POM，引入spring-cloud-starter-alibaba-nacos-discovery和spring-cloud-starter-alibaba-sentinel依赖
					<dependency>
						<groupId>com.alibaba.cloud</groupId>
						<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
					</dependency>
					<dependency>
						<groupId>com.alibaba.cloud</groupId>
						<artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
					</dependency>
				c. 写YML
					server:
					  port: 9003

					spring:
					  application:
						name: nacos-payment-provider
					  cloud:
						nacos:
						  discovery:
							server-addr: 192.168.107.6:1111
						sentinel:
						  transport:
							dashboard: localhost:8089
							port: 8719

					management:
					  endpoints:
						web:
						  exposure:
							include: '*'
				d. 主启动
					@SpringBootApplication
					@EnableDiscoveryClient
					public class NacosPaymentMain9003 {

						public static void main(String[] args) {
							SpringApplication.run(NacosPaymentMain9003.class, args);
						}
					}
				e. 业务类：
				@RestController
				public class PaymentController {
					@Value("${server.port}")
					private String serverPort;
					private static Map<Long, Payment> map = new HashMap<>();
					static {
						map.put(1L, new Payment(1L, "921127"));
						map.put(2L, new Payment(2L, "940906"));
						map.put(3L, new Payment(3L, "960504"));
					}
					
					@GetMapping("/payment/{id}")
					public CommonResult<Payment> getPaymentById(@PathVariable("id") Long id) {
						Payment payment = map.get(id);
						return new CommonResult<>(200, "from mysql, serverPort：" + serverPort, payment);
					}
				}
				f. 测试地址：http://localhost:9003/payment/1 / http://localhost:9004/payment/1
			C. 消费者cloudalibaba-consumer-nacos-order80
				a. 新建cloudalibaba-consumer-nacos-order80
				b. 改POM，引入spring-cloud-starter-alibaba-nacos-discovery和spring-cloud-starter-alibaba-sentinel依赖
					<dependency>
						<groupId>com.alibaba.cloud</groupId>
						<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
					</dependency>
					<dependency>
						<groupId>com.alibaba.cloud</groupId>
						<artifactId>spring-cloud-starter-alibaba-sentinel</artifactId>
					</dependency>
				c. 写YML
					server:
					  port: 80

					spring:
					  application:
						name: nacos-order-consumer
					  cloud:
						nacos:
						  discovery:
							server-addr: 192.168.107.6:1111
						sentinel:
						  transport:
							dashboard: localhost:8089
							port: 8719

					service-url:
					  nacos-user-service: http://nacos-payment-provider
				d. 主启动
					@SpringBootApplication
					@EnableDiscoveryClient
					public class OrderSentinelMain80 {
						public static void main(String[] args) {
							SpringApplication.run(OrderSentinelMain80.class, args);
						}
					}
				e. 业务类
					@Configuration
					public class ApplicationContextConfig {
						@Bean
						@LoadBalanced
						public RestTemplate restTemplate() {
							return new RestTemplate();
						}
					}
				f. CircleBreakerController
					(1) 修改后重启微服务
						(A) 热部署对java代码生效及时
						(B) 对@SentinelResource注解内属性，有时生效，有时不生效
					(2) 目的
						(A) fallback管运行时异常
						(B) blockHandler管配置违规
					(3) 测试地址：http://localhost/consumer/payment/1
					(4) 没有配置：访问出错，返回错误页面，不友好
					(5) 只配置fallback
						@RestController
						@Slf4j
						public class CircleBreakerController {

							@Value("${service-url.nacos-user-service}")
							private String serverUrl;

							@Autowired
							private RestTemplate restTemplate;
					
						@SentinelResource(value = "fallback", fallback = "handlerFallback")
						public CommonResult<Payment> getPaymentById(@PathVariable("id") Long id) {
						
						public CommonResult<Payment> handlerFallback(Long id, Throwable e) {
						Payment payment = new Payment(id, null);
						return new CommonResult<>(404, "handlerFallback, exception: " + e.getMessage(), payment);
						}
					(6) 只配置blockHandler
						@SentinelResource(value = "fallback", blockHandler = "blockHandler")
						public CommonResult<Payment> getPaymentById(@PathVariable("id") Long id) {
						
						public CommonResult<Payment> blockHandler(Long id, BlockException e) {
							Payment payment = new Payment(id, null);
							return new CommonResult<>(404, "blockHandler, exception: " + e.getMessage(), payment);
						}
					(7) fallback和blockHandler都配置
						(A) 代码：
							@SentinelResource(value = "fallback", fallback = "handlerFallback", blockHandler = "blockHandler")
							public CommonResult<Payment> getPaymentById(@PathVariable("id") Long id) {
						(B) 结果：若blockHandler和fallback都进行了配置。则被限流降级而抛出的BlockException时，只会进入blockHandler处理逻辑
					(8) 忽略属性
						(A) 代码：
							@SentinelResource(value = "fallback", fallback = "handlerFallback", blockHandler = "blockHandler",
							exceptionsToIgnore = {IllegalArgumentException.class})
							public CommonResult<Payment> getPaymentById(@PathVariable("id") Long id) {
						(B) 结果：没有服务降级，返回错误页面
						(C) 结论：返回错误页面，不友好
		③ Feign系列
			A. 修改消费者cloudalibaba-consumer-nacos-order80
				a. 80消费者调用提供者9003
				b. Feign组件一般是消费侧
			B. 改POM，新增spring-cloud-starter-openfeign依赖
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-openfeign</artifactId>
				</dependency>
			C. 写YML，新增配置
				feign:
				  sentinel:
					enabled: true
			D. 主启动类：新增@EnableFeignClients注解
			E. 业务类：
				a. 带@FeignClient注解的业务接口
					@FeignClient(value = "nacos-payment-provider", fallback = PaymentFallbackService.class)
					public interface PaymentService {

						@GetMapping("/payment/{id}")
						public CommonResult<Payment> getPaymentById(@PathVariable("id") Long id);
					}
				b. fallback=PaymentFallbackService
					@Component
					public class PaymentFallbackService implements PaymentService {

						@Override
						public CommonResult<Payment> getPaymentById(Long id) {
							Payment payment = new Payment(id, null);
							return new CommonResult<>(404, "fallback, exception: 服务降级返回", payment);
						}
					}
				c. controller
					@Autowired
					private PaymentService paymentService;
					
					@GetMapping("/consumer/feign/payment/{id}")
					public CommonResult<Payment> getPaymentById(@PathVariable("id") Long id) {
						return paymentService.getPaymentById(id);
					}
			F. 测试地址：http://localhost/consumer/feign/payment/1
			G. 关闭9003和9004微服务提供者，80消费者侧自动降级，服务不会被卡死
		④ 熔断框架比较
			------------------------------------------------------------------------------------------------------
			|				 | Sentinel							| Hystrix				| resilience4j			 |
			|----------------|----------------------------------|-----------------------|------------------------|
			| 隔离策略		 | 信号量隔离（并发线程限流）		| 线程池隔离/信号量隔离 | 信号量隔离			 |
			| 熔断降级策略	 | 基于响应时间、异常比例数、异常数 | 基于异常比例			| 基于异常比例、响应时间 |
			| 实时统计实现	 | 滑动窗口（LeapArray）			| 滑动窗口（基于RxJava）| Ring Bit Buffer		 |
			| 动态规则配置	 | 支持多种数据源					| 支持多种数据源		| 有限支持				 |
			| 扩展性		 | 多个扩展点						| 插件的形式			| 接口模式				 |
			| 基于注解的支持 | 支持								| 支持					| 支持					 |
			| 限流			 | 基于QPS，支持基于调用关系的限流	| 有限的支持			| Rate Limit			 |
			------------------------------------------------------------------------------------------------------
	11. 规则持久化
		① 一旦重启应用，sentinel规则将消失，生产环境需要将配置规则进行持久化
		② 将限流配置规则持久化进Nacos保存，只要刷新8401某个rest地址，sentinel控制台的流控规则就能看到，只要Nacos里面的配置不删除，针对8401上的流控规则持续有效
		③ 步骤
			A. 修改cloudalibaba-sentinel-service8401
			B. 改POM，引入依赖sentinel-datasource-nacos
				<dependency>
					<groupId>com.alibaba.csp</groupId>
					<artifactId>sentinel-datasource-nacos</artifactId>
				</dependency>
			C. 写YML，添加Nacos数据源配置
				spring:
				  application:
					name: cloudalibaba-sentinel-service
				  cloud:
					nacos:
					  discovery:
						server-addr: 192.168.107.6:1111
					sentinel:
					  transport:
						dashboard: localhost:8089
						port: 8719
					  datasource:
						ds1:
						  nacos:
							server-addr: ${spring.cloud.nacos.discovery.server-addr}
							dataId: ${spring.application.name}
							groupId: DEFAULT_GROUP
							data-type: json
							rule-type: flow
			D. 添加Nacos业务规则配置
				a. 新建配置管理：
					(1) Data Id对应spring.application.name属性
					(2) group对应spring.cloud.nacos.config.group属性
					(3) 配置格式：json
					(4) 配置内容：
						[
							{
								"resource": "/testB", 
								"limitApp": "default", 
								"grade": 1, 
								"count": 1, 
								"strategy": 0, 
								"controlBehavior": 0, 
								"clusterMode": false
							}
						]
				b. 内容解析：
					(1) resource：资源名称
					(2) limitApp：来源应用
					(3) grade：阈值类型，0代表线程数，1代表QPS
					(4) count：单机阈值
					(5) strategy：流控模式，0代表直接，1代表关联，2代表链路
					(6) controlBehavior：流控效果，0代表快速失败，1代表Warm Up，2代表排队等待
					(7) clusterMode：是否集群
			E. 重启8401，发现Sentinel上已存在持久化的流控规则	
			F. 快速访问接口：http://localhost:8401/testB，配置生效，服务被限流，报默认的限流异常Blocked by Sentinel (flow limiting)
			G. 重启8401，Sentinel上的流控规则仍然存在
二十、Spring Cloud Alibaba Seata处理分布式事务
	1. 分布式事务问题
		① 分布式前单机单库不存在该问题
		② 分布式之后
			A. 单体应用被拆分成微服务应用，原来的三个模块被拆分成三个独立的应用，分别使用三个独立的数据库，业务操作需要调用三个服务来完成。此时每个服务内部的数据一致性由本地事务来保证，
			但是全局的数据一致性问题没法保证。
			B. 用户购买商品的业务逻辑。整个业务逻辑由3个微服务提供支持
				a. 仓储服务：对给定的商品和出仓储数量
				b. 订单服务：根据采购需求创建订单
				c. 账户服务：从用户账户中扣除余额
			C. 架构图
									RPC   -----------		 ----------
								  |-----> | Storage | -----> |   DB	  |
				------------	  |		  -----------		 ----------
				| Business | -----|
				------------	  |	RPC	  -----------  RPC	 -----------
								  |-----> |  Order 	| -----> | Account |
										  -----------		 -----------
											   ↑				  ↑
											   ↓				  ↓
										  -----------		 ------------
										  |    DB	|		 |    DB	|
										  -----------		 ------------
		③ 一句话：一次业务操作需要跨多个数据源或需要跨多个系统进行远程调用，就会产生分布式事务问题
	2. Seata简介
		① Seata：
			A. Seata 是一款开源的分布式事务解决方案，致力于在微服务架构下提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。
			B. 官网地址：http://seata.io/zh-cn/
		② 作用：一个典型的分布式事务过程
			A. 分布式事务处理过程的一ID+三组件模型
				a. Transaction ID XID：全局唯一的事务ID
				b. 三组件概念：
					(1) TC（Transaction Coordinator）：事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚
					(2) TM（Transaction Manager）：控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚决议。
					(3) RM（Resource Manager）：控制分支事务，负责分支注册、状态汇报、并接受事务协调器的指令，驱动分支（本地）事务的提交和回滚
			B. 处理过程
				a. TM 向 TC申请开启一个全局事务，全局事务创建成功并生成一个全局唯一XID
				b. XID 在微服务调用链路的上下文中传播
				c. RM 向 TC 注册分支事务，将其纳入XID对应全局事务的管辖
				d. TM 向 TC 发起针对XID的全局提交或者回滚决议
				e. TC 调度XID下管辖的全部分支事务完成提交或回滚请求
				---------------------   Begin/Commit/Rollback						   		------
				| Microservice | TM | <---------------------------------------------------->|	 |
				|--------------------	Register Branch								   XID 	|	 |
				|  RM  |  RM  | <-|-------------------------------------------------------->|	 |
				---------------   |	   ----------------				Branch Commit/Rollback  |	 |
				   ↓	   ↓      |--> | Microservice |										|	 |
				------   ------ 	   |---------------										|	 |
				| DB |   | DB |	 	   |  RM  | RM |<-|----------------------------------- >|	 |
				------	 ------		   -------------  |	   ----------------					|	 |
										  ↓		 ↓	  |--> | Microservice |					| TC |
										------ ------	   |--------------|					|	 |
										| DB | | DB |	   |  RM   |  RM  | <-------------->|	 |
										------ ------	   ----------------					|	 |
															   ↓	   ↓					|	 |
															------   ------					|	 |
															| DB |	 | DB |					|	 |
															------	 ------					|	 |
																							------
		③ 下载网址：https://github.com/seata/seata/releases
		④ 使用：
			A. 本地@Transaction
			B. 全局@GlobalTransaction：只需要使用@GlobalTransaction标注在业务方法之上
	3. Seata-Server安装
		① 官网地址：http://seata.io/zh-cn/
		② 下载版本：seata-server-1.0.0.zip
		③ 解压seata-server-1.0.0.zip到指定的目录并修改conf目录下的file.conf配置文件
			A. 先备份原始file.conf文件
			B. 主要修改：自定义事务组名称+事务日志存储模式为db+数据库连接信息
			C. file.conf
				a. service模块：将vgroup_mapping.my_test_tx_group = "default"改为vgroup_mapping.my_test_tx_group = "seata_tx_group"
				b. store模块：
					(1) 将mode = "file"改为mode = "db"
					(2) 数据库连接信息
						db {
							## the implement of javax.sql.DataSource, such as DruidDataSource(druid)/BasicDataSource(dbcp) etc.
							datasource = "dbcp"
							## mysql/oracle/h2/oceanbase etc.
							db-type = "mysql"
							driver-class-name = "com.mysql.jdbc.Driver"
							url = "jdbc:mysql://192.168.107.6:3306/seata"
							user = "root"
							password = "2648"
						  }
		④ mysql5.7数据库新建seata
		⑤ 在seata库中建表，建表db_store.sql在seata\conf目录里面
		⑥ 修改seata\conf目录里面的registry.conf配置文件
			A. 先备份原始registry.conf文件
			B. 主要修改：registry的type属性，nacos的serverAddr属性
			C. registry.conf
				a. registry模块：将type = "file"改为type = "nacos"
				b. nacos模块：将serverAddr = "localhost1"改为serverAddr = "192.167.107.6:1111"
			D. 目的是：指明注册中心为nacos，及修改nacos连接信息
		⑦ 先启动Nacos
		⑧ 再启动seata-server：执行seata\bin\seata-server.bat
	4. 订单/库存/账户业务数据库准备
		① 以下压实都需要先启动Nacos后启动Seata
		② 分布式事务业务说明
			A. 创建是三个微服务：订单服务、库存服务和账户服务，当用户下单时，会在订单服务中创建一个订单，然后通过远程调用服务扣减下单商品的库存，再通过远程调用账户服务来扣减用户账户里面的余额，
			最后在订单服务中修改订单状态为已完成。
			B. 操作跨越三个数据库，有两次远程调用，很明显会有分布式问题
			C. 下订单->减库存->扣账户（余额）-改（订单）状态
		③ 创建业务数据库
			A. seata_order：存储订单的数据库
			B. seata_storage：存储库存的数据库
			C. seata_account：存储账户信息的数据库
		④ 按照上述三个数据库分别创建对应的业务表
			A. seata_order库下创建order表
				CREATE TABLE `order` (
				  `id` bigint(11) NOT NULL AUTO_INCREMENT,
				  `user_id` bigint(11) DEFAULT NULL COMMENT '用户ID',
				  `product_id` bigint(11) DEFAULT NULL COMMENT '产品ID',
				  `count` int(11) DEFAULT NULL COMMENT '数量',
				  `money` double(11,0) DEFAULT NULL COMMENT '余额',
				  `status` int(1) DEFAULT NULL COMMENT '订单状态：0代表创建中；1代表已完结',
				  PRIMARY KEY (`id`)
				) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
			B. seata_storage库下创建storage表
				CREATE TABLE `storage` (
				  `id` bigint(11) NOT NULL AUTO_INCREMENT,
				  `product_id` bigint(11) DEFAULT NULL COMMENT '产品ID',
				  `total` int(11) DEFAULT NULL COMMENT '总库存',
				  `used` int(11) DEFAULT NULL COMMENT '已用库存',
				  `residue` int(11) DEFAULT NULL COMMENT '剩余库存',
				  PRIMARY KEY (`id`)
				) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

				insert  into `storage`(`id`,`product_id`,`total`,`used`,`residue`) values (1,1,100,0,100);
			C. seata_account库下创建account表
				CREATE TABLE `account` (
				  `id` bigint(11) NOT NULL AUTO_INCREMENT,
				  `user_id` bigint(11) DEFAULT NULL COMMENT '用户ID',
				  `total` double(10,0) DEFAULT NULL COMMENT '总余额',
				  `user` double(10,0) DEFAULT NULL COMMENT '已用余额',
				  `residue` double(10,0) DEFAULT NULL COMMENT '剩余可用余额',
				  PRIMARY KEY (`id`)
				) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

				insert  into `account`(`id`,`user_id`,`total`,`user`,`residue`) values (1,1,1000,0,1000);
		⑤ 按照上述三个数据库分别创建对应的回滚日志表，该建表SQL的路径为：seata\conf\db_undo_log.sql	
	5. 订单/库存/账户业务微服务准备
		① 业务需求：下订单->减库存->扣账户（余额）-改（订单）状态
		② 新建订单Order-Module
			A. 新建Module：seata-order-service2001
			B. 改POM，引入spring-cloud-starter-alibaba-seata依赖和与安装的seata对应版本的seata依赖，并排除spring-cloud-starter-alibaba-seata依赖的seata-all
				<dependency>
					<groupId>com.alibaba.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-nacos-discovery</artifactId>
				</dependency>
				<dependency>
					<groupId>com.alibaba.cloud</groupId>
					<artifactId>spring-cloud-starter-alibaba-seata</artifactId>
					<exclusions>
						<exclusion>
							<groupId>io.seata</groupId>
							<artifactId>seata-all</artifactId>
						</exclusion>
					</exclusions>
				</dependency>
				<dependency>
					<groupId>io.seata</groupId>
					<artifactId>seata-all</artifactId>
					<version>1.0.0</version>
				</dependency>
				<dependency>
					<groupId>org.springframework.cloud</groupId>
					<artifactId>spring-cloud-starter-openfeign</artifactId>
				</dependency>
			C. 写YML
				server:
				  port: 2001

				spring:
				  application:
					name: seata-order-service
				  cloud:
					alibaba:
					  seata:
						# 自定义事务组名称需要与seata-server的file.conf中的file.conf属性保持一致
						tx-service-group: seata_tx_group
					nacos:
					  discovery:
						server-addr: 192.168.107.6:1111
				  datasource:
					driver-class-name: com.mysql.jdbc.Driver
					url: jdbc:mysql://192.168.107.6:3306/seata_order
					username: root
					password: 2648

				feign:
				  hystrix:
					enabled: false

				logging:
				  level: info

				mybatis:
				  mapper-locations: classpath:mapper/*.xml
				  type-aliases-package: com.li.springcloud.entities
			D. 新建file.conf和registry.conf，内容复制seata\conf\file.conf和seata\conf\registry.conf的内容
			E. 统一在cloud-api-commons创建与数据表对应的entity类
			F. Dao接口及实现
				a. OrderDao
					@Mapper
					public interface OrderDao {

						// 新建订单
						void create(Order order);

						// 修改订单状态，从 0 改为 1
						void update(@Param("userId") Long userId, @Param("status") Integer status);
					}
				b. resource文件夹下新建mapper文件夹后添加OrderMapper.xml文件
					<?xml version="1.0" encoding="UTF-8" ?>
					<!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
					<mapper namespace="com.li.springcloud.OrderDao">

						<insert id="create">
							INSERT INTO order(user_id, product_id, count, money, status)
							VALUES (#{userId}, #{productId}, #{count}, #{money}, 0)
						</insert>
						
						<update id="update">
							UPDATE order SET status = 1 where user_id = #{userId} and status = #{status}
						</update>
					</mapper>
			G. service接口及实现
				public interface OrderService {

					// 新建订单
					void create(Order order);
				}
				
				@FeignClient(value = "seata-storage-service")
				public interface StorageService {

					@PostMapping("/storage/decrease")
					CommonResult decrease(@RequestParam("productId") Long productId, @RequestParam("count") Integer count);
				}
				
				@FeignClient(value = "seata-account-service")
				public interface AccountService {

					@PostMapping("/account/decrease")
					CommonResult decrease(@RequestParam("useId") Long useId, @RequestParam("money") BigDecimal money);
				}
			H. controller
				@RestController
				public class OrderController {

					@Autowired
					private OrderService orderService;

					@GetMapping("/order/create")
					public CommonResult<Order> create(Order order) {
						orderService.create(order);
						return new CommonResult<>(200,"订单创建完成");
					}
				}
			I. config
				a. MyBatisConfig
					@Configuration
					@MapperScan("{com.li.springcloud.dao}")
					public class MyBatisConfig {
					}
				b. DataSourceProxyConfig
					@Configuration
					public class DataSourceProxyConfig {

						@Value("${mybatis.mapper-locations}")
						private String mapperLocation;

						@Bean
						@ConfigurationProperties(prefix = "spring.datasource")
						public DataSource druidDataSource() {
							return new DruidDataSource();
						}

						@Bean
						public DataSourceProxy dataSourceProxy(DataSource dataSource) {
							return new DataSourceProxy(dataSource);
						}

						public SqlSessionFactory sqlSessionFactory(DataSourceProxy dataSourceProxy) throws Exception {
							SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();
							sqlSessionFactoryBean.setDataSource(dataSourceProxy);
							sqlSessionFactoryBean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(mapperLocation));
							sqlSessionFactoryBean.setTransactionFactory(new SpringManagedTransactionFactory());
							return sqlSessionFactoryBean.getObject();
						}
					}
			J. 主启动：SeataOrderMain2001
				@EnableFeignClients
				@EnableDiscoveryClient
				@SpringBootApplication(exclude = DataSourceAutoConfiguration.class)// 取消数据源的自动创建
				public class SeataOrderMain2001 {

					public static void main(String[] args) {
						SpringApplication.run(SeataOrderMain2001.class, args);
					}
				}
		③ 新建库存Storage-Module，该创建过程以及代码与订单Order-Module基本一致
			A. 新建Module：seata-storage-service2002
			B. 写YML
				server:
				  port: 2002

				spring:
				  application:
					name: seata-storage-service
			C. Dao接口及实现
				a. StorageDao
					@Mapper
					public interface StorageDao {

						void decrease(@Param("productId") Long productId, @Param("count") Integer count);
					}
				b. StorageMapper
					<mapper namespace="com.li.springcloud.dao.StorageDao">

						<update id="decrease">
							UPDATE storage SET used = used + #{count}, residue = residue - #{count} WHERE product_id = #{productId}
						</update>
					</mapper>
			D. service接口及实现
				a. StorageService
					public interface StorageService {

						void decrease(Long productId, Integer count);
					}
				b. StorageServiceImpl
					@Service
					public class StorageServiceImpl implements StorageService {

						@Autowired
						private StorageDao storageDao;

						@Override
						public void decrease(Long productId, Integer count) {
							storageDao.decrease(productId, count);
						}
					}
			E. controller
				@RestController
				public class StorageController {

					@Autowired
					private StorageService storageService;

					@GetMapping("/storage/decrease")
					public CommonResult decrease(@RequestParam("productId") Long productId, @RequestParam("count") Integer count) {
						storageService.decrease(productId, count);
						return new CommonResult(200, "修改库存成功");
					}
				}
			F. 主启动：SeataStorageMain2002
		④ 新建账户Account-Module，该创建过程以及代码与库存Storage-Module和订单Order-Module基本一致
			A. 新建Module：seata-account-service2003
			B. 写YML
				server:
				  port: 2003

				spring:
				  application:
					name: seata-account-service
			C. Dao接口及实现
				a. AccountDao
					@Mapper
					public interface AccountDao {

						void decrease(@Param("useId") Long useId, @Param("money") BigDecimal money);
					}
				b. AccountMapper
					<mapper namespace="com.li.springcloud.dao.AccountDao">

						<update id="decrease">
							update account set used = used + #{money}, residue = residue + #{money} where user_id = #{useId}
						</update>
					</mapper>
			D. service接口及实现
				a. AccountService
					public interface AccountService {

						void decrease(Long useId, BigDecimal money);
					}
				b. AccountServiceImpl
					@Service
					public class AccountServiceImpl implements AccountService {

						@Autowired
						private AccountDao accountDao;

						@Override
						public void decrease(Long useId, BigDecimal money) {
							accountDao.decrease(useId, money);
						}
					}
			E. controller
				@RestController
				public class AccountController {

					@Autowired
					private AccountService accountService;

					@GetMapping("/account/decrease")
					public CommonResult decrease(@RequestParam("useId") Long useId, @RequestParam("money") BigDecimal money){
						accountService.decrease(useId, money);
						return new CommonResult(200, "修改余额完成");
					}
				}
			F. 主启动：SeataAccountMain2003
	6. Test
		① 正常下单：http://localhost:2001/order/create?userId=1&productId=1&count=10&money=100.0
		② 没加@GlobalTransactional
			A. 给AccountServiceImpl添加超时
			B. 数据库情况，微服务2003发生超时异常故而余额扣除失败，而订单创建和库存扣除成功，但是订单状态为0
			C. 故障情况：
				a. 当库存和账户金额扣减后，订单状态并没有设置为已经完成，没有从0该为1
				b. 由于feign的重试机制，账户余额还可能多次扣减
		③ 添加@GlobalTransactional
			A. 给AccountServiceImpl添加超时
			B. 在OrderServiceImpl中添加@GlobalTransactional(name = "seata-create-order", rollbackFor = Exception.class)
			C. 下单后数据库数据没有任何变化，没有记录插进数据库，同时没有任何数据发生变化
	7. Seata原理简介
		① Seata：
			A. 2019年1月蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案
			B. Simple Extensible Autonomous Transaction Architecture，简单可扩展自治事务框架
			C. 2020起始，参加工作后用1.0以后的版本
		② TC、TM、RM三大组件：
			A. TC：seata服务器
			B. TM：事务的发起方，标注了@GlobalTransactional注解服务
			C. RM：事务的参与方，一个RM代表一个数据库
			D. 分布式事务的执行流程：
				a. TM开启分布式事务（TM向TC注册全局事务记录）
				b. 按业务场景、编排数据库、服务等事务内资源（RM向TC汇报资源准备状态）
				c. TM结束分布式事务，事务一阶段结束（TM通知TC提交/回滚分布式事务）
				d. TC汇总事务信息，决定分布式事务是提交还是回滚
				e. TC通知所有RM提交/回滚资源，事务二阶段结束
		③ AT模式如何做到对业务的无侵入：
			A. AT模式：
			B. 一阶段加载：
				a. 业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。
				b. 在一阶段，Seata会拦截“业务SQL”
					(1) 解析SQL语句，找到“业务SQL”要更新的业务数据，在业务数据更新前，将其保存到“before image”
					(2) 执行“业务SQL”更新业务数据，在业务数据更新之后，将其保存到“after image”，最后生成行锁
					(3) 以上操作全部在一个数据库事务中完成，这样保证了一阶段操作的原子性
														   ------------------------------------------------------------------------------------------------------------
					-----------		  ----------------	   |	  --------------------------	   ---------------		 -------------------------		 ------------ |
					| 业务SQL | ----> | 提取表元数据 | ----|----> | 保存原快照before image | ----> | 执行业务SQL | ----> | 保存新快照after image | ----> | 生成行锁 | | ---->
					-----------		  ----------------	   |	  --------------------------	   ---------------		 -------------------------		 ------------ |
														   ------------------------------------------------------------------------------------------------------------
														   ----------------------------------------------			|
														   |   		---------- ---------- ---------- 	|			| 提交业务SQL、undo/redo log、行锁
														   | 业务DB | 业务表 | | log 表 | | Lock表 |	| <---------|
														   |   		---------- ---------- ---------- 	|
														   ----------------------------------------------
			C. 二阶段提交
				a. 二阶段如果是顺利提交的话，因为“业务SQL”在一阶段已经提交至数据库，所以Seata框架只需要将一阶段保存的快照数据和行锁删除，完成数据清理即可
				b. 示意图：			
					---------------------	--------------------	------------
					| 删除 before image |	| 删除 after image |	| 删除行锁 |
					---------------------	--------------------	------------
							|						 |					 |
							----------------------------------------------
													 |
												-----------
												| DB 事务 |
												-----------
													 ↓
							-----------------------------------------------------						
							|	----------- ---------- --------- ----------		|
							|	| 业务表1 | |业务表2 | | log表 | | lock表 |		|
							|	----------- ---------- --------- ----------		|
							-----------------------------------------------------
			D. 二阶段回滚
				a. 二阶段如果是回滚的话，Seata就需要回滚一阶段已经执行的“业务SQL”，还原业务数据；
				b. 回滚方式便是用“before image”还原业务数据，但在还原前要首先检验脏写，对比“数据库当前业务数据”和“after image”，如果两份数据完全一致就说明没有脏写，可以还原业务数据，
				如果不一致就说明有脏写，出现脏写就需要转人工处理
							----------------	-----------------------------------------------
					 |---->	|   检验脏写   |	|        “after image” vs “数据库数据”        |
					 |		----------------	-----------------------------------------------
				DB	 |		----------------	-----------------------------------------------
				事   |---->	|   还原数据   |	|  “before image” -> “逆向SQL” -> “数据还原”  |
				务	 |      ----------------	-----------------------------------------------
				|	 |		----------------	-----------------------------------------------
				|	 |---->	| 删除中间数据 |	| 删除before image、删除after image、删除行锁 |
				|			----------------	-----------------------------------------------
				|			---------------------------------------------------
				|			|	----------- ---------- --------- ----------	  |
				|---------> |	| 业务表1 | |业务表2 | | log表 | | lock表 |	  |
							|	-----------	---------- --------- ----------   |
							---------------------------------------------------

